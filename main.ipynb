{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dcbcac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ashish is great\n"
     ]
    }
   ],
   "source": [
    "print(\"ashish is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a30ccaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] embeddings (text-embedding-004) working\n",
      "[ok] LLM (gemini-2.0-flash) ready\n",
      "[ok] Imports + key setup complete (no google.generativeai used)\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: Imports + Gemini setup (LangChain Google GenAI ONLY) ====\n",
    "\n",
    "# core libs you asked for\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import WebBaseLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# langchain google genai provider\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# env + key\n",
    "import os\n",
    "API_KEY = \"AIzaSyByRrLMm4_UhLecvKKsJOPp_M76YVwjyuk\"  # <-- your key\n",
    "\n",
    "if not API_KEY.strip():\n",
    "    raise RuntimeError(\"Set API_KEY first.\")\n",
    "\n",
    "# set both names some libs look for\n",
    "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
    "os.environ[\"GEMINI_API_KEY\"] = API_KEY\n",
    "# avoid ADC fallback\n",
    "os.environ.pop(\"GOOGLE_APPLICATION_CREDENTIALS\", None)\n",
    "\n",
    "# model handles (LangChain Google GenAI ONLY)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, api_key=API_KEY)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=API_KEY)\n",
    "\n",
    "# quick sanity check (no google.generativeai used)\n",
    "try:\n",
    "    _ = embedding_model.embed_query(\"hello world\")\n",
    "    print(\"[ok] embeddings (text-embedding-004) working\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Embeddings init failed: {e}\")\n",
    "\n",
    "print(\"[ok] LLM (gemini-2.0-flash) ready\")\n",
    "print(\"[ok] Imports + key setup complete (no google.generativeai used)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea8861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] STATE initialized â€” run_id=00de0447b3d8 â†’ artifacts: tmp\\00de0447b3d8\n",
      "[ok] Prompts ready (escaped correctly). Next: loadâ†’chunkâ†’FAISSâ†’detect in Cell 3.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 2: State + Options + Prompts (no executions yet) ====\n",
    "# Sets up:\n",
    "#   â€¢ Test inputs (RESUME_FILE / RESUME_TEXT, JD_FILE / JD_TEXT)\n",
    "#   â€¢ Run options/thresholds\n",
    "#   â€¢ Flexible STATE dict + artifact paths\n",
    "#   â€¢ Core prompts (resume detector + section classifier + extractors)\n",
    "# Nothing heavy runs here; next cell will load, chunk, embed, and detect.\n",
    "\n",
    "# stdlib\n",
    "import json, re, uuid\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List, TypedDict\n",
    "\n",
    "# ---------- Test inputs (edit these) ----------\n",
    "RESUME_FILE: Optional[str] = \"JALADI ASHISH RESUME_oracle.pdf\"                 # e.g., \"samples/alice_resume.pdf\"\n",
    "RESUME_TEXT: Optional[str] = None                 # paste resume text if no file\n",
    "\n",
    "JD_FILE: Optional[str] = None                     # e.g., \"samples/senior_de_jd.txt\"\n",
    "JD_TEXT: Optional[str] = \"\"\"\\\n",
    "ðŸ· Job Title\n",
    "\n",
    "Associate Software Engineer\n",
    "\n",
    "ðŸ“‹ Role Summary\n",
    "\n",
    "As an Associate Software Engineer, you will work closely under the guidance of senior engineers to design, develop, test, and maintain software applications. This role is ideal for early-career developers who are proactive, willing to learn, and ready to contribute to production code.\n",
    "\n",
    "ðŸ”§ Responsibilities\n",
    "\n",
    "Assist in the analysis, design, development, and implementation of software modules and features\n",
    "\n",
    "Write clean, maintainable, and efficient code following best practices\n",
    "\n",
    "Participate in code reviews and give/receive feedback to ensure code quality\n",
    "\n",
    "Debug, troubleshoot, and fix defects in existing applications\n",
    "\n",
    "Create and execute unit tests to validate code correctness\n",
    "\n",
    "Contribute to system documentation including API specs, architecture diagrams, and user manuals\n",
    "\n",
    "Collaborate with cross-functional teams (product, QA, UX) to gather requirements and support feature delivery\n",
    "\n",
    "Adhere to software development lifecycle (SDLC) practices: planning, development, testing, deployment\n",
    "\n",
    "Stay up to date with emerging technologies and propose improvements to the tech stack\n",
    "\n",
    "ðŸŽ¯ Qualifications / Skills Required\n",
    "\n",
    "Bachelorâ€™s degree in Computer Science, Software Engineering, or a related field\n",
    "\n",
    "0â€“2 years of software development experience (including internships, coop, or academic projects)\n",
    "\n",
    "Proficiency in one or more programming languages (e.g. Python, Java, C++, JavaScript)\n",
    "\n",
    "Good understanding of data structures, algorithms, and object-oriented programming\n",
    "\n",
    "Experience with version control systems (e.g. Git)\n",
    "\n",
    "Familiarity with RESTful APIs, web frameworks, or backend services\n",
    "\n",
    "Basic knowledge of relational databases (SQL)\n",
    "\n",
    "Strong analytical and problem-solving skills\n",
    "\n",
    "Good communication and teamwork skills\n",
    "\n",
    "Eagerness to learn, take feedback, and grow technically\n",
    "\n",
    "ðŸŒŸ Preferred / Nice-to-Have\n",
    "\n",
    "Knowledge of unit testing frameworks, test automation\n",
    "\n",
    "Exposure to cloud services or DevOps tooling (AWS, Docker, CI/CD pipelines)\n",
    "\n",
    "Experience with frontend frameworks (React, Angular, Vue)\n",
    "\n",
    "Understanding of NoSQL databases\n",
    "\n",
    "Experience with Agile / Scrum methodologies\n",
    "\"\"\".strip()\n",
    "\n",
    "# ---------- Options / thresholds ----------\n",
    "class RunOptions(TypedDict):\n",
    "    chunk_tokens: int\n",
    "    chunk_overlap: float\n",
    "    faiss_topk: int\n",
    "    match_strong: int\n",
    "    match_partial: int\n",
    "    recency_months: int\n",
    "    cap_if_must_have_missing: int\n",
    "    seniority_penalty: int\n",
    "    degree_penalty: int\n",
    "    strict_mode: bool\n",
    "    language: str\n",
    "\n",
    "DEFAULT_OPTIONS: RunOptions = {\n",
    "    \"chunk_tokens\": 1000,\n",
    "    \"chunk_overlap\": 0.18,\n",
    "    \"faiss_topk\": 12,\n",
    "    \"match_strong\": 85,\n",
    "    \"match_partial\": 65,\n",
    "    \"recency_months\": 30,\n",
    "    \"cap_if_must_have_missing\": 69,\n",
    "    \"seniority_penalty\": 5,\n",
    "    \"degree_penalty\": 8,\n",
    "    \"strict_mode\": True,\n",
    "    \"language\": \"en\",\n",
    "}\n",
    "\n",
    "# ---------- STATE (flexible bag-of-facts) ----------\n",
    "class PipelineState(TypedDict, total=False):\n",
    "    run_id: str\n",
    "    options: RunOptions\n",
    "    flags: Dict[str, bool]\n",
    "\n",
    "    inputs: Dict[str, Optional[str]]\n",
    "    raw: Dict[str, Optional[str]]\n",
    "\n",
    "    provenance: Dict[str, Any]\n",
    "    chunks: List[Dict[str, Any]]\n",
    "    faiss: Dict[str, Any]\n",
    "\n",
    "    contacts: Dict[str, Any]\n",
    "    high_level: Dict[str, Any]\n",
    "\n",
    "    education: List[Dict[str, Any]]\n",
    "    timeline: List[Dict[str, Any]]\n",
    "    projects: List[Dict[str, Any]]\n",
    "    skills: List[Dict[str, Any]]\n",
    "    certs: List[str]\n",
    "    awards: List[str]\n",
    "\n",
    "    jd_snapshot: Dict[str, Any]\n",
    "    canon: Dict[str, Any]\n",
    "    jd_alignment: Dict[str, Any]\n",
    "\n",
    "    coverage: Dict[str, Any]\n",
    "    final: Dict[str, Any]\n",
    "\n",
    "    artifacts: Dict[str, Any]\n",
    "    audit: List[str]\n",
    "\n",
    "def new_state(\n",
    "    resume_file: Optional[str],\n",
    "    resume_text: Optional[str],\n",
    "    jd_file: Optional[str],\n",
    "    jd_text: Optional[str],\n",
    "    options: RunOptions = DEFAULT_OPTIONS,\n",
    ") -> PipelineState:\n",
    "    run_id = uuid.uuid4().hex[:12]\n",
    "    base_dir = Path(f\"./tmp/{run_id}\")\n",
    "    paths = {\n",
    "        \"base_dir\": str(base_dir),\n",
    "        \"chunks_json\": str(base_dir / \"chunks.json\"),\n",
    "        \"entities_by_chunk_json\": str(base_dir / \"entities_by_chunk.json\"),\n",
    "        \"merged_entities_json\": str(base_dir / \"merged_entities.json\"),\n",
    "        \"canon_map_json\": str(base_dir / \"canon_map.json\"),\n",
    "        \"coverage_json\": str(base_dir / \"coverage.json\"),\n",
    "        \"final_json\": str(base_dir / \"final.json\"),\n",
    "        \"faiss_dir\": str(base_dir / \"faiss\"),\n",
    "    }\n",
    "\n",
    "    st: PipelineState = {\n",
    "        \"run_id\": run_id,\n",
    "        \"options\": options,\n",
    "        \"flags\": {\"is_resume\": True},  # will be set by detector in next cell\n",
    "\n",
    "        \"inputs\": {\n",
    "            \"resume_file\": resume_file,\n",
    "            \"resume_text\": resume_text,\n",
    "            \"jd_file\": jd_file,\n",
    "            \"jd_text\": jd_text,\n",
    "        },\n",
    "        \"raw\": {\"resume_text\": None, \"jd_text\": None},\n",
    "\n",
    "        \"provenance\": {\"chunks\": []},\n",
    "        \"chunks\": [],\n",
    "        \"faiss\": {\"index_path\": None, \"topk_ids\": []},\n",
    "\n",
    "        \"contacts\": {\"name\": None, \"email\": None, \"phone\": None,\n",
    "                     \"links\": {\"linkedin\": None, \"github\": None, \"portfolio\": None, \"website\": None}},\n",
    "        \"high_level\": {\"summary\": None, \"location\": None, \"years_experience\": None},\n",
    "\n",
    "        \"education\": [],\n",
    "        \"timeline\": [],\n",
    "        \"projects\": [],\n",
    "        \"skills\": [],\n",
    "        \"certs\": [],\n",
    "        \"awards\": [],\n",
    "\n",
    "        \"jd_snapshot\": {\"title\": None, \"required\": [], \"preferred\": [], \"responsibilities\": []},\n",
    "        \"canon\": {\"skill_map\": {}, \"normalized_skills\": [], \"normalized_required\": [], \"normalized_preferred\": []},\n",
    "        \"jd_alignment\": {\"required\": [], \"preferred\": []},\n",
    "\n",
    "        \"coverage\": {},\n",
    "        \"final\": {},\n",
    "\n",
    "        \"artifacts\": {\"base_dir\": paths[\"base_dir\"], \"paths\": paths},\n",
    "        \"audit\": [],\n",
    "    }\n",
    "    return st\n",
    "\n",
    "STATE = new_state(RESUME_FILE, RESUME_TEXT, JD_FILE, JD_TEXT)\n",
    "print(f\"[ok] STATE initialized â€” run_id={STATE['run_id']} â†’ artifacts: {STATE['artifacts']['base_dir']}\")\n",
    "\n",
    "# ---------- Prompts ----------\n",
    "PROMPTS: Dict[str, str] = {}\n",
    "\n",
    "# Helper to escape all braces except the variables we actually pass\n",
    "def _escape_braces_keep_vars(template: str, keep_vars: List[str]) -> str:\n",
    "    esc = template.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    for v in keep_vars:\n",
    "        esc = esc.replace(\"{{\" + v + \"}}\", \"{\" + v + \"}\")\n",
    "    return esc\n",
    "\n",
    "PROMPTS[\"resume_detector\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "You are a strict document classifier. Decide if the uploaded content is a *resume/CV*.\n",
    "Positive resume signals (examples, not variables): {{title, company, dates, location}}, {{degree, field, institution, year}}.\n",
    "\n",
    "Return **JSON only**:\n",
    "{\n",
    "  \"label\": \"resume\" | \"close_to_resume\" | \"not_resume\",\n",
    "  \"confidence\": 0.0 to 1.0,\n",
    "  \"reasons\": [\"short bullets\"],\n",
    "  \"quick_metadata\": {\n",
    "    \"has_contact_like_section\": true|false,\n",
    "    \"has_experience_like_section\": true|false,\n",
    "    \"has_education_like_section\": true|false,\n",
    "    \"has_skills_like_section\": true|false,\n",
    "    \"pages_seen\": <int>,\n",
    "    \"approx_length_chars\": <int>\n",
    "  }\n",
    "}\n",
    "\n",
    "Text to classify:\n",
    "---\n",
    "{resume_excerpt}\n",
    "---\n",
    "(Only the JSON object; no extra text.)\n",
    "\"\"\".strip(), [\"resume_excerpt\"])\n",
    "\n",
    "PROMPTS[\"section_classifier\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "Label the resume chunk as one of:\n",
    "  \"contact\" | \"summary\" | \"education\" | \"experience\" | \"projects\" | \"skills\" | \"certifications\" | \"awards\" | \"other\"\n",
    "\n",
    "Return JSON only:\n",
    "{ \"label\": \"<one_of_the_labels>\", \"rationale\": \"<very_short_reason>\" }\n",
    "\n",
    "Chunk:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\"\"\".strip(), [\"chunk_text\"])\n",
    "\n",
    "PROMPTS[\"extract_experience\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "Extract **verbatim** experience entries in this chunk.\n",
    "Return JSON array of:\n",
    "  { \"title\": \"\", \"company\": \"\", \"location\": null, \"start\": null, \"end\": null,\n",
    "    \"highlights\": [], \"evidence\": \"<short verbatim>\", \"chunk_id\": \"<id>\" }\n",
    "\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\"\"\".strip(), [\"chunk_id\", \"chunk_text\"])\n",
    "\n",
    "PROMPTS[\"extract_skills\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "List explicit skills/technologies/frameworks/clouds/databases.\n",
    "Return JSON array of:\n",
    "  { \"name\": \"<as_written>\", \"aliases\": [], \"evidence\": [\"<short verbatim>\"], \"chunk_id\": \"<id>\" }\n",
    "\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\"\"\".strip(), [\"chunk_id\", \"chunk_text\"])\n",
    "\n",
    "PROMPTS[\"extract_education\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "Extract education facts in this chunk.\n",
    "Return JSON array of:\n",
    "  { \"degree\": \"\", \"field\": \"\", \"institution\": \"\", \"start\": null, \"end\": null,\n",
    "    \"evidence\": \"<short verbatim>\", \"chunk_id\": \"<id>\" }\n",
    "\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\"\"\".strip(), [\"chunk_id\", \"chunk_text\"])\n",
    "\n",
    "PROMPTS[\"extract_projects\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "Extract projects/initiatives if present.\n",
    "Return JSON array of:\n",
    "  { \"name\": \"\", \"tech\": [], \"impact\": null, \"links\": [], \"chunk_id\": \"<id>\" }\n",
    "\n",
    "Chunk ID: {chunk_id}\n",
    "Chunk:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\"\"\".strip(), [\"chunk_id\", \"chunk_text\"])\n",
    "\n",
    "PROMPTS[\"jd_snapshot\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "From the JD, extract:\n",
    "- title (short)\n",
    "- required (skill tokens; lowercase)\n",
    "- preferred (skill tokens; lowercase)\n",
    "- responsibilities (short verbs/n-grams; lowercase)\n",
    "\n",
    "Return JSON only:\n",
    "{ \"title\": \"\", \"required\": [], \"preferred\": [], \"responsibilities\": [] }\n",
    "\n",
    "JD:\n",
    "---\n",
    "{jd_text}\n",
    "---\n",
    "\"\"\".strip(), [\"jd_text\"])\n",
    "\n",
    "print(\"[ok] Prompts ready (escaped correctly). Next: loadâ†’chunkâ†’FAISSâ†’detect in Cell 3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83182b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Chunked resume into 2 chunks â†’ tmp\\00de0447b3d8\\chunks.json\n",
      "[ok] FAISS index saved at tmp\\00de0447b3d8\\faiss\n",
      "[ok] Top-2 JD-relevant chunks: ['c_01', 'c_00']\n",
      "[ok] Detector label: resume (confidence=0.95)\n",
      "[ok] Classified as resume/close_to_resume â†’ proceed to extraction pipeline next.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 3: Load â†’ chunk â†’ embed (FAISS) â†’ detect resume/not_resume ====\n",
    "\n",
    "from pathlib import Path\n",
    "import json, re, zipfile, xml.etree.ElementTree as ET\n",
    "\n",
    "# ---------- helpers: read files or raw text ----------\n",
    "def read_pdf_pymupdf(path: str) -> str:\n",
    "    \"\"\"Use PyMuPDFLoader (no fitz import needed here).\"\"\"\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    return \"\\n\".join(d.page_content or \"\" for d in docs)\n",
    "\n",
    "def read_docx_quick(path: str) -> str:\n",
    "    \"\"\"Lightweight .docx text reader via zip/XML (no python-docx dependency).\"\"\"\n",
    "    with zipfile.ZipFile(path) as z:\n",
    "        xml_bytes = z.read(\"word/document.xml\")\n",
    "    root = ET.fromstring(xml_bytes)\n",
    "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "    lines = []\n",
    "    for p in root.findall(\".//w:p\", ns):\n",
    "        txt = \"\".join((t.text or \"\") for t in p.findall(\".//w:t\", ns)).strip()\n",
    "        if txt:\n",
    "            lines.append(txt)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def read_text_file(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def read_any(path_or_text: Optional[str]) -> str:\n",
    "    \"\"\"If file exists, read by extension; else treat as raw text.\"\"\"\n",
    "    if not path_or_text:\n",
    "        return \"\"\n",
    "    p = Path(path_or_text)\n",
    "    if p.exists():\n",
    "        ext = p.suffix.lower()\n",
    "        if ext == \".pdf\":\n",
    "            return read_pdf_pymupdf(str(p))\n",
    "        if ext == \".docx\":\n",
    "            return read_docx_quick(str(p))\n",
    "        return read_text_file(str(p))\n",
    "    return str(path_or_text)\n",
    "\n",
    "# ---------- ensure artifacts dir ----------\n",
    "base_dir = Path(STATE[\"artifacts\"][\"base_dir\"])\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- load resume & JD into STATE[\"raw\"] ----------\n",
    "resume_text = STATE[\"raw\"].get(\"resume_text\") or (\n",
    "    read_any(STATE[\"inputs\"].get(\"resume_file\")) if STATE[\"inputs\"].get(\"resume_file\") else read_any(STATE[\"inputs\"].get(\"resume_text\"))\n",
    ")\n",
    "jd_text = STATE[\"raw\"].get(\"jd_text\") or (\n",
    "    read_any(STATE[\"inputs\"].get(\"jd_file\")) if STATE[\"inputs\"].get(\"jd_file\") else read_any(STATE[\"inputs\"].get(\"jd_text\"))\n",
    ")\n",
    "\n",
    "if not resume_text.strip():\n",
    "    raise RuntimeError(\"No resume content found. Provide RESUME_FILE or RESUME_TEXT in Cell 2.\")\n",
    "if not jd_text.strip():\n",
    "    raise RuntimeError(\"No JD content found. Provide JD_FILE or JD_TEXT in Cell 2.\")\n",
    "\n",
    "STATE[\"raw\"][\"resume_text\"] = resume_text\n",
    "STATE[\"raw\"][\"jd_text\"] = jd_text\n",
    "\n",
    "# ---------- chunking (char-based proxy for tokens) ----------\n",
    "opts = STATE[\"options\"]\n",
    "chunk_chars = int(opts[\"chunk_tokens\"] * 4)         # â‰ˆ 4 chars/token\n",
    "overlap_chars = int(chunk_chars * opts[\"chunk_overlap\"])\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_chars,\n",
    "    chunk_overlap=overlap_chars,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "chunks_texts: List[str] = splitter.split_text(resume_text)\n",
    "\n",
    "STATE[\"chunks\"].clear()\n",
    "for i, txt in enumerate(chunks_texts):\n",
    "    cid = f\"c_{i:02d}\"\n",
    "    STATE[\"chunks\"].append({\n",
    "        \"id\": cid,\n",
    "        \"page\": None,        # optional page mapping later\n",
    "        \"text\": txt,\n",
    "        \"start\": None,\n",
    "        \"end\": None,\n",
    "        \"tokens\": None,      # we chunked by chars; token count optional\n",
    "        \"label\": None,\n",
    "    })\n",
    "\n",
    "with open(STATE[\"artifacts\"][\"paths\"][\"chunks_json\"], \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(STATE[\"chunks\"], f, ensure_ascii=False, indent=2)\n",
    "print(f\"[ok] Chunked resume into {len(STATE['chunks'])} chunks â†’ {STATE['artifacts']['paths']['chunks_json']}\")\n",
    "\n",
    "# ---------- embeddings + FAISS (LangChain Google GenAI ONLY) ----------\n",
    "texts = [c[\"text\"] for c in STATE[\"chunks\"]]\n",
    "metas = [{\"id\": c[\"id\"]} for c in STATE[\"chunks\"]]\n",
    "\n",
    "# use the embedding_model handle from Cell 1\n",
    "vs = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metas)\n",
    "\n",
    "faiss_dir = Path(STATE[\"artifacts\"][\"paths\"][\"faiss_dir\"])\n",
    "faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "vs.save_local(str(faiss_dir))\n",
    "STATE[\"faiss\"][\"index_path\"] = str(faiss_dir)\n",
    "print(f\"[ok] FAISS index saved at {STATE['faiss']['index_path']}\")\n",
    "\n",
    "# JD-guided retrieval (optional prioritization)\n",
    "topk = max(1, int(opts[\"faiss_topk\"]))\n",
    "docs = vs.similarity_search(jd_text[:4000], k=min(topk, len(STATE[\"chunks\"])))\n",
    "STATE[\"faiss\"][\"topk_ids\"] = [d.metadata.get(\"id\") for d in docs if d.metadata and d.metadata.get(\"id\")]\n",
    "\n",
    "print(f\"[ok] Top-{len(STATE['faiss']['topk_ids'])} JD-relevant chunks: {STATE['faiss']['topk_ids']}\")\n",
    "\n",
    "# ---------- resume/not_resume detector ----------\n",
    "def _json_loose(s: str) -> Dict[str, Any]:\n",
    "    s = (s or \"\").strip()\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "        if m:\n",
    "            return json.loads(m.group(0))\n",
    "        raise ValueError(\"Detector did not return valid JSON.\")\n",
    "\n",
    "# Build an excerpt (first 1â€“2 chunks or raw)\n",
    "excerpt = \"\\n\\n\".join(c[\"text\"] for c in STATE[\"chunks\"][: min(2, len(STATE[\"chunks\"]))]) or resume_text[:3500]\n",
    "\n",
    "detector_prompt = ChatPromptTemplate.from_template(PROMPTS[\"resume_detector\"])\n",
    "detector_chain = detector_prompt | llm\n",
    "detector_raw = detector_chain.invoke({\"resume_excerpt\": excerpt})\n",
    "detector_out = detector_raw.content if hasattr(detector_raw, \"content\") else str(detector_raw)\n",
    "\n",
    "try:\n",
    "    det = _json_loose(detector_out)\n",
    "except Exception:\n",
    "    det = {\n",
    "        \"label\": \"close_to_resume\",\n",
    "        \"confidence\": 0.3,\n",
    "        \"reasons\": [\"LLM parsing fallback\"],\n",
    "        \"quick_metadata\": {\n",
    "            \"has_contact_like_section\": False,\n",
    "            \"has_experience_like_section\": False,\n",
    "            \"has_education_like_section\": False,\n",
    "            \"has_skills_like_section\": False,\n",
    "            \"pages_seen\": None,\n",
    "            \"approx_length_chars\": len(resume_text),\n",
    "        },\n",
    "    }\n",
    "\n",
    "label = str(det.get(\"label\", \"close_to_resume\")).lower().strip()\n",
    "STATE[\"flags\"][\"is_resume\"] = label in {\"resume\", \"close_to_resume\"}\n",
    "STATE[\"audit\"].append(f\"resume_detector.label={label}\")\n",
    "print(f\"[ok] Detector label: {label} (confidence={det.get('confidence')})\")\n",
    "\n",
    "# ---------- early exit if NOT a resume ----------\n",
    "if label == \"not_resume\":\n",
    "    STATE[\"final\"] = {\n",
    "        \"score_100\": 0,\n",
    "        \"breakdown\": {},\n",
    "        \"reasons\": [],          # per your spec: no remarks\n",
    "        \"strong_matches\": [],\n",
    "        \"skill_gaps\": [],\n",
    "        \"risk_flags\": []\n",
    "    }\n",
    "    with open(STATE[\"artifacts\"][\"paths\"][\"final_json\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(STATE[\"final\"], f, ensure_ascii=False, indent=2)\n",
    "    print(\"[END] Classified as not_resume â†’ score=0, no remarks. Stopping here for this run.\")\n",
    "else:\n",
    "    print(\"[ok] Classified as resume/close_to_resume â†’ proceed to extraction pipeline next.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "648c15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] JD snapshot: {'title': 'Data Analyst', 'required': ['analytical skills', 'problem-solving skills', 'sql', 'excel', 'python', 'r', 'data visualization', 'power bi', 'tableau', 'google data studio', 'statistics', 'data modeling', 'data cleaning', 'communication skills'], 'preferred': ['etl pipelines', 'data warehouses', 'bigquery', 'snowflake', 'redshift', 'machine learning', 'predictive modeling', 'apis', 'google analytics', 'crm systems'], 'responsibilities': ['collect data', 'clean data', 'validate data', 'perform data analysis', 'identify trends', 'identify patterns', 'identify anomalies', 'develop dashboards', 'develop reports', 'query data', 'manipulate data', 'data wrangling', 'data analysis', 'automation', 'translate data insights', 'support a/b testing', 'performance tracking', 'ad-hoc analysis', 'ensure data quality', 'ensure data accuracy', 'ensure data consistency']}\n",
      "[ok] Router decisions (num chunks flagged): {'contacts': 1, 'education': 2, 'experience': 1, 'projects': 2, 'skills': 2}\n",
      "[ok] Router-driven extraction complete.\n",
      "[ok] Counts â†’ education: 8 | experience: 1 | projects: 7 | skills (uniq): 48\n",
      "[ok] Contacts: {'name': 'JALADI ASHISH', 'email': 'ashujaladi5995@gmail.com', 'phone': '+91 9493110947', 'links': {'linkedin': 'https://www.linkedin.com/in/ashish-jaladi-046a3b24a/', 'github': None, 'portfolio': None, 'website': None}}\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 4 (updated): JD snapshot â†’ LLM chunk routing (no hardcoded sections) â†’ per-extractor parsing ====\n",
    "# What changed:\n",
    "#  â€¢ Replaced the old \"section_classifier\" with an LLM **chunk_router** that decides which extractors to run\n",
    "#    for each chunk (contacts / education / experience / projects / skills) based on semantics, not headings.\n",
    "#  â€¢ Contacts are collected from chunks the router marks as contacts; regex still used for precision.\n",
    "#  â€¢ Everything else (JD snapshot, extractors, provenance, light de-dup) stays aligned with our plan.\n",
    "\n",
    "import re, json\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _escape_braces_keep_vars(template: str, keep_vars: List[str]) -> str:\n",
    "    esc = template.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    for v in keep_vars:\n",
    "        esc = esc.replace(\"{{\" + v + \"}}\", \"{\" + v + \"}\")\n",
    "    return esc\n",
    "\n",
    "def _json_loose(s: str) -> Any:\n",
    "    s = (s or \"\").strip()\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}|\\[.*\\]\", s, flags=re.S)\n",
    "        if m:\n",
    "            return json.loads(m.group(0))\n",
    "        raise\n",
    "\n",
    "def _norm_tokens(xs: List[str]) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for x in xs or []:\n",
    "        s = (x or \"\").strip().lower()\n",
    "        if s and s not in out:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# Safety: ensure we didn't early-exit as not_resume\n",
    "if STATE.get(\"final\", {}).get(\"score_100\") == 0 and not STATE[\"flags\"].get(\"is_resume\", True):\n",
    "    raise RuntimeError(\"This run ended earlier as not_resume. Start a new run with a valid resume.\")\n",
    "\n",
    "# ---------- 1) JD snapshot (same as before) ----------\n",
    "jd_prompt = ChatPromptTemplate.from_template(PROMPTS[\"jd_snapshot\"])\n",
    "jd_chain = jd_prompt | llm\n",
    "jd_raw = jd_chain.invoke({\"jd_text\": STATE[\"raw\"][\"jd_text\"]})\n",
    "try:\n",
    "    jd_obj = _json_loose(getattr(jd_raw, \"content\", str(jd_raw)))\n",
    "except Exception:\n",
    "    jd_obj = {\"title\": None, \"required\": [], \"preferred\": [], \"responsibilities\": []}\n",
    "\n",
    "STATE[\"jd_snapshot\"][\"title\"] = (jd_obj.get(\"title\") or \"\").strip() or STATE[\"jd_snapshot\"][\"title\"]\n",
    "STATE[\"jd_snapshot\"][\"required\"] = _norm_tokens(jd_obj.get(\"required\", []))\n",
    "STATE[\"jd_snapshot\"][\"preferred\"] = _norm_tokens(jd_obj.get(\"preferred\", []))\n",
    "STATE[\"jd_snapshot\"][\"responsibilities\"] = _norm_tokens(jd_obj.get(\"responsibilities\", []))\n",
    "STATE[\"audit\"].append(\"jd_snapshot.ok\")\n",
    "print(\"[ok] JD snapshot:\", STATE[\"jd_snapshot\"])\n",
    "\n",
    "# ---------- 2) LLM router (semantic, not headings) ----------\n",
    "# New router prompt: for a given chunk, tell which extractors apply (true/false) with confidence & notes.\n",
    "PROMPTS[\"chunk_router\"] = _escape_braces_keep_vars(r\"\"\"\n",
    "You act as a semantic router for resume chunks. Do **not** rely on literal headings alone.\n",
    "Infer the chunkâ€™s purpose from content and phrasing (e.g., duties, achievements, dates, technologies, institutions).\n",
    "\n",
    "For this chunk, decide which extractors should run (true/false):\n",
    "- contacts: email/phone/links (LinkedIn/GitHub/portfolio/website), candidate name lines.\n",
    "- education: degrees, fields, institutions, years.\n",
    "- experience: role titles, companies, date ranges, locations, bullet achievements, impact/metrics.\n",
    "- projects: named projects/initiatives with tech stacks and outcomes; internships count if project-oriented.\n",
    "- skills: explicit lists or inline mentions of technologies, tools, clouds, databases, frameworks, languages.\n",
    "\n",
    "Return JSON **only**:\n",
    "{\n",
    "  \"apply\": {\n",
    "    \"contacts\": true|false,\n",
    "    \"education\": true|false,\n",
    "    \"experience\": true|false,\n",
    "    \"projects\": true|false,\n",
    "    \"skills\": true|false\n",
    "  },\n",
    "  \"confidence\": 0.0 to 1.0,\n",
    "  \"notes\": \"brief rationale\"\n",
    "}\n",
    "\n",
    "Chunk:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\"\"\".strip(), [\"chunk_text\"])\n",
    "\n",
    "router_chain = ChatPromptTemplate.from_template(PROMPTS[\"chunk_router\"]) | llm\n",
    "\n",
    "router_stats = {\"contacts\":0,\"education\":0,\"experience\":0,\"projects\":0,\"skills\":0}\n",
    "for ch in STATE[\"chunks\"]:\n",
    "    out = router_chain.invoke({\"chunk_text\": ch[\"text\"]})\n",
    "    try:\n",
    "        r = _json_loose(getattr(out, \"content\", str(out)))\n",
    "        apply = r.get(\"apply\", {}) or {}\n",
    "    except Exception:\n",
    "        apply = {}\n",
    "    # normalize booleans\n",
    "    for k in [\"contacts\",\"education\",\"experience\",\"projects\",\"skills\"]:\n",
    "        apply[k] = bool(apply.get(k, False))\n",
    "        if apply[k]:\n",
    "            router_stats[k] += 1\n",
    "    ch[\"route\"] = apply\n",
    "\n",
    "print(\"[ok] Router decisions (num chunks flagged):\", router_stats)\n",
    "\n",
    "# ---------- 3) Per-extractor LLM chains (same extractor prompts as Cell 2) ----------\n",
    "exp_chain  = ChatPromptTemplate.from_template(PROMPTS[\"extract_experience\"]) | llm\n",
    "skill_chain= ChatPromptTemplate.from_template(PROMPTS[\"extract_skills\"])     | llm\n",
    "edu_chain  = ChatPromptTemplate.from_template(PROMPTS[\"extract_education\"])  | llm\n",
    "proj_chain = ChatPromptTemplate.from_template(PROMPTS[\"extract_projects\"])   | llm\n",
    "\n",
    "# Speed: prioritize FAISS top-K chunks (JD-relevant); still LLM decides which extractors apply.\n",
    "use_only_topk = True\n",
    "topk_ids = set(STATE[\"faiss\"][\"topk_ids\"]) if use_only_topk else None\n",
    "\n",
    "# ---------- 4) Contacts collection (regex over LLM-marked contact chunks) ----------\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "PHONE_RE = re.compile(r\"(?:(?:\\+\\d{1,3}[\\s\\-]?)?(?:\\(?\\d{3}\\)?[\\s\\-]?\\d{3}[\\s\\-]?\\d{4}))\", re.I)\n",
    "LINKEDIN_RE = re.compile(r\"(https?://(?:www\\.)?linkedin\\.com/in/[A-Za-z0-9\\-_/]+)\", re.I)\n",
    "GITHUB_RE = re.compile(r\"(https?://(?:www\\.)?github\\.com/[A-Za-z0-9\\-_/]+)\", re.I)\n",
    "GEN_URL_RE = re.compile(r\"(https?://[^\\s]+)\", re.I)\n",
    "\n",
    "def _apply_contacts_from_text(text: str):\n",
    "    email = next(iter(EMAIL_RE.findall(text)), None)\n",
    "    phone = next(iter(PHONE_RE.findall(text)), None)\n",
    "    if email and not STATE[\"contacts\"].get(\"email\"):\n",
    "        STATE[\"contacts\"][\"email\"] = email\n",
    "    if phone and not STATE[\"contacts\"].get(\"phone\"):\n",
    "        STATE[\"contacts\"][\"phone\"] = phone\n",
    "    li = LINKEDIN_RE.search(text)\n",
    "    gh = GITHUB_RE.search(text)\n",
    "    if li and not STATE[\"contacts\"][\"links\"].get(\"linkedin\"):\n",
    "        STATE[\"contacts\"][\"links\"][\"linkedin\"] = li.group(1)\n",
    "    if gh and not STATE[\"contacts\"][\"links\"].get(\"github\"):\n",
    "        STATE[\"contacts\"][\"links\"][\"github\"] = gh.group(1)\n",
    "    # generic portfolio/website, avoid duplicating linkedin/github\n",
    "    for u in GEN_URL_RE.findall(text):\n",
    "        if \"linkedin.com\" in u or \"github.com\" in u:\n",
    "            continue\n",
    "        if not STATE[\"contacts\"][\"links\"].get(\"portfolio\"):\n",
    "            STATE[\"contacts\"][\"links\"][\"portfolio\"] = u\n",
    "            break\n",
    "\n",
    "contact_text = \"\"\n",
    "for i, ch in enumerate(STATE[\"chunks\"]):\n",
    "    if ch.get(\"route\", {}).get(\"contacts\", False):\n",
    "        # respect topK speed gate if enabled\n",
    "        if topk_ids and ch[\"id\"] not in topk_ids:\n",
    "            continue\n",
    "        contact_text += \"\\n\" + ch[\"text\"]\n",
    "# fallback: use first two chunks if router found nothing\n",
    "if not contact_text.strip() and STATE[\"chunks\"]:\n",
    "    for ch in STATE[\"chunks\"][:2]:\n",
    "        contact_text += \"\\n\" + ch[\"text\"]\n",
    "_apply_contacts_from_text(contact_text)\n",
    "\n",
    "# Naive name guess: first plausible line in any contact-routed chunk\n",
    "def _guess_name(chunks: List[Dict[str,Any]]) -> Optional[str]:\n",
    "    for ch in chunks:\n",
    "        if not ch.get(\"route\", {}).get(\"contacts\", False):\n",
    "            continue\n",
    "        for line in ch[\"text\"].splitlines():\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            low = s.lower()\n",
    "            if any(x in low for x in [\"email\", \"phone\", \"linkedin\", \"github\", \"@\", \"http\", \"www.\"]):\n",
    "                continue\n",
    "            if 2 <= len(s.split()) <= 7 and 3 <= len(s) <= 60:\n",
    "                return s\n",
    "    return None\n",
    "\n",
    "if not STATE[\"contacts\"].get(\"name\"):\n",
    "    nm = _guess_name(STATE[\"chunks\"])\n",
    "    if nm:\n",
    "        STATE[\"contacts\"][\"name\"] = nm\n",
    "\n",
    "# ---------- 5) Extraction loop driven by router.apply booleans ----------\n",
    "entities_by_chunk: Dict[str, Dict[str, Any]] = {}\n",
    "for ch in STATE[\"chunks\"]:\n",
    "    cid, txt, route = ch[\"id\"], ch[\"text\"], ch.get(\"route\") or {}\n",
    "    if topk_ids and cid not in topk_ids:\n",
    "        continue  # skip non-topK chunks for speed\n",
    "\n",
    "    entities_by_chunk[cid] = {\"experience\": [], \"skills\": [], \"education\": [], \"projects\": []}\n",
    "\n",
    "    if route.get(\"experience\"):\n",
    "        out = exp_chain.invoke({\"chunk_id\": cid, \"chunk_text\": txt})\n",
    "        try:\n",
    "            arr = _json_loose(getattr(out, \"content\", str(out)))\n",
    "            if isinstance(arr, list):\n",
    "                entities_by_chunk[cid][\"experience\"].extend(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if route.get(\"skills\"):\n",
    "        out = skill_chain.invoke({\"chunk_id\": cid, \"chunk_text\": txt})\n",
    "        try:\n",
    "            arr = _json_loose(getattr(out, \"content\", str(out)))\n",
    "            if isinstance(arr, list):\n",
    "                entities_by_chunk[cid][\"skills\"].extend(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if route.get(\"education\"):\n",
    "        out = edu_chain.invoke({\"chunk_id\": cid, \"chunk_text\": txt})\n",
    "        try:\n",
    "            arr = _json_loose(getattr(out, \"content\", str(out)))\n",
    "            if isinstance(arr, list):\n",
    "                entities_by_chunk[cid][\"education\"].extend(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if route.get(\"projects\"):\n",
    "        out = proj_chain.invoke({\"chunk_id\": cid, \"chunk_text\": txt})\n",
    "        try:\n",
    "            arr = _json_loose(getattr(out, \"content\", str(out)))\n",
    "            if isinstance(arr, list):\n",
    "                entities_by_chunk[cid][\"projects\"].extend(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Persist raw entities\n",
    "Path(STATE[\"artifacts\"][\"base_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "with open(STATE[\"artifacts\"][\"paths\"][\"entities_by_chunk_json\"], \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(entities_by_chunk, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ---------- 6) Flatten + light de-dup ----------\n",
    "for cid, groups in entities_by_chunk.items():\n",
    "    for it in groups.get(\"education\", []):\n",
    "        it.setdefault(\"chunk_id\", cid)\n",
    "        STATE[\"education\"].append(it)\n",
    "\n",
    "    for it in groups.get(\"experience\", []):\n",
    "        it.setdefault(\"chunk_id\", cid)\n",
    "        if \"highlights\" not in it or it[\"highlights\"] is None:\n",
    "            it[\"highlights\"] = []\n",
    "        STATE[\"timeline\"].append(it)\n",
    "\n",
    "    for it in groups.get(\"projects\", []):\n",
    "        it.setdefault(\"chunk_id\", cid)\n",
    "        if \"tech\" not in it or it[\"tech\"] is None:\n",
    "            it[\"tech\"] = []\n",
    "        if \"links\" not in it or it[\"links\"] is None:\n",
    "            it[\"links\"] = []\n",
    "        STATE[\"projects\"].append(it)\n",
    "\n",
    "    for it in groups.get(\"skills\", []):\n",
    "        nm = (it.get(\"name\") or \"\").strip().lower()\n",
    "        if not nm:\n",
    "            continue\n",
    "        aliases = [a.strip().lower() for a in (it.get(\"aliases\") or []) if a]\n",
    "        ev = it.get(\"evidence\") or []\n",
    "        STATE[\"skills\"].append({\n",
    "            \"name\": nm,\n",
    "            \"aliases\": aliases,\n",
    "            \"evidence\": ev,\n",
    "            \"chunk_ids\": [cid],\n",
    "            \"frequency\": 1,\n",
    "            \"recent_use_years\": None\n",
    "        })\n",
    "\n",
    "# Collapse duplicates by exact lowercased name\n",
    "skill_index: Dict[str, Dict[str, Any]] = {}\n",
    "collapsed: List[Dict[str, Any]] = []\n",
    "for s in STATE[\"skills\"]:\n",
    "    nm = s[\"name\"]\n",
    "    if nm in skill_index:\n",
    "        tgt = skill_index[nm]\n",
    "        tgt[\"frequency\"] += 1\n",
    "        for e in s[\"evidence\"]:\n",
    "            if e and e not in tgt[\"evidence\"]:\n",
    "                tgt[\"evidence\"].append(e)\n",
    "        for cc in s[\"chunk_ids\"]:\n",
    "            if cc not in tgt[\"chunk_ids\"]:\n",
    "                tgt[\"chunk_ids\"].append(cc)\n",
    "        for a in s[\"aliases\"]:\n",
    "            if a and a not in tgt[\"aliases\"]:\n",
    "                tgt[\"aliases\"].append(a)\n",
    "    else:\n",
    "        skill_index[nm] = s\n",
    "        collapsed.append(s)\n",
    "STATE[\"skills\"] = collapsed\n",
    "\n",
    "STATE[\"audit\"].append(\"router_extraction.ok\")\n",
    "print(\"[ok] Router-driven extraction complete.\")\n",
    "print(\"[ok] Counts â†’ education:\", len(STATE[\"education\"]), \n",
    "      \"| experience:\", len(STATE[\"timeline\"]), \n",
    "      \"| projects:\", len(STATE[\"projects\"]), \n",
    "      \"| skills (uniq):\", len(STATE[\"skills\"]))\n",
    "print(\"[ok] Contacts:\", STATE[\"contacts\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb137a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Final score: 20/100\n",
      "[ok] Breakdown: {'must_have_coverage': 7.5, 'preferred_alignment': 2.25, 'experience_seniority_fit': 1.13, 'responsibility_overlap': 0.0, 'evidence_depth': 4.0, 'education_cert_match': 5.0}\n",
      "[ok] Reasons:\n",
      "  - Strong matches on required: sql, python, power bi\n",
      "  - Experience: 0.17 yrs vs JD ~3 yrs (fit=6%).\n",
      "  - Responsibilities overlap: 0% ; Evidence signals: 4 metrics found.\n",
      "[ok] Risk flags:\n",
      "  - Missing required: analytical skills, problem solving skills, excel, r, data visualization, tableauâ€¦\n",
      "[ok] Saved â†’ tmp\\00de0447b3d8\\final.json\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 5: Merge â†’ JD-aware normalization â†’ coverage features â†’ final 0â€“100 score (+save) ====\n",
    "# No new deps; only stdlib. Uses the STATE built by previous cells and `llm` handle if needed later.\n",
    "\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "import math, json, re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- helpers (stdlib only) ----------\n",
    "def _clean_token(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    # normalize common punctuation variants: node.js -> nodejs, c++ -> cpp, .net -> dotnet\n",
    "    s = s.replace(\"+\", \"p\").replace(\".js\", \"js\").replace(\".net\", \"dotnet\")\n",
    "    s = re.sub(r\"[^a-z0-9#]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _sim(a: str, b: str) -> int:\n",
    "    # similarity 0..100 using difflib (no rapidfuzz to avoid extra deps)\n",
    "    return int(round(100 * SequenceMatcher(None, a, b).ratio()))\n",
    "\n",
    "def _best_match(token: str, candidates: list[str]) -> tuple[Optional[str], int]:\n",
    "    token_c = _clean_token(token)\n",
    "    best = None\n",
    "    best_sc = -1\n",
    "    for c in candidates:\n",
    "        sc = _sim(token_c, _clean_token(c))\n",
    "        if sc > best_sc:\n",
    "            best_sc = sc\n",
    "            best = c\n",
    "    return best, best_sc\n",
    "\n",
    "_MONTHS = {\n",
    "    \"jan\":1,\"january\":1,\"feb\":2,\"february\":2,\"mar\":3,\"march\":3,\"apr\":4,\"april\":4,\n",
    "    \"may\":5,\"jun\":6,\"june\":6,\"jul\":7,\"july\":7,\"aug\":8,\"august\":8,\"sep\":9,\"sept\":9,\"september\":9,\n",
    "    \"oct\":10,\"october\":10,\"nov\":11,\"november\":11,\"dec\":12,\"december\":12\n",
    "}\n",
    "\n",
    "def _parse_date(s: Optional[str]) -> Optional[datetime]:\n",
    "    if not s: return None\n",
    "    t = s.strip().lower()\n",
    "    t = re.sub(r\"[^\\w\\s\\-\\/]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    # current / present\n",
    "    if any(x in t for x in [\"present\", \"current\", \"now\"]):\n",
    "        return datetime.today()\n",
    "    # YYYY-MM or MM/YYYY or Mon YYYY or YYYY\n",
    "    m = re.search(r\"(\\d{4})[-/](\\d{1,2})\", t)\n",
    "    if m:\n",
    "        y, mo = int(m.group(1)), int(m.group(2))\n",
    "        mo = 1 if mo < 1 or mo > 12 else mo\n",
    "        return datetime(y, mo, 1)\n",
    "    m = re.search(r\"(\\d{1,2})[-/](\\d{4})\", t)\n",
    "    if m:\n",
    "        mo, y = int(m.group(1)), int(m.group(2))\n",
    "        mo = 1 if mo < 1 or mo > 12 else mo\n",
    "        return datetime(y, mo, 1)\n",
    "    m = re.search(r\"([A-Za-z]{3,9})\\s+(\\d{4})\", t)\n",
    "    if m:\n",
    "        mo = _MONTHS.get(m.group(1)[:3], 1)\n",
    "        y  = int(m.group(2))\n",
    "        return datetime(y, mo, 1)\n",
    "    m = re.search(r\"\\b(20\\d{2}|19\\d{2})\\b\", t)\n",
    "    if m:\n",
    "        y = int(m.group(1))\n",
    "        return datetime(y, 1, 1)\n",
    "    return None\n",
    "\n",
    "def _months_between(a: Optional[datetime], b: Optional[datetime]) -> int:\n",
    "    if not a or not b: return 0\n",
    "    months = (b.year - a.year) * 12 + (b.month - a.month)\n",
    "    return max(0, months)\n",
    "\n",
    "def _count_numbers(texts: list[str]) -> int:\n",
    "    cnt = 0\n",
    "    for t in texts:\n",
    "        for num in re.findall(r\"\\b\\d+(?:\\.\\d+)?%?|\\b\\d+[kKmMbB]?\\b\", t or \"\"):\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "def _unique(seq):\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "# ---------- 1) Merge roles, compute experience years ----------\n",
    "now = datetime.today()\n",
    "total_months = 0\n",
    "for role in STATE[\"timeline\"]:\n",
    "    st = _parse_date(role.get(\"start\"))\n",
    "    en = _parse_date(role.get(\"end\")) or now\n",
    "    if st:\n",
    "        total_months += _months_between(st, en)\n",
    "\n",
    "years_experience = round(total_months / 12.0, 2) if total_months > 0 else None\n",
    "STATE[\"high_level\"][\"years_experience\"] = years_experience\n",
    "\n",
    "# ---------- 2) Skill normalization (build canon from JD req+pref) ----------\n",
    "jd_req = STATE[\"jd_snapshot\"].get(\"required\", []) or []\n",
    "jd_pref = STATE[\"jd_snapshot\"].get(\"preferred\", []) or []\n",
    "canon_pool = _unique([_clean_token(s) for s in (jd_req + jd_pref) if s])\n",
    "\n",
    "resume_skill_names = []\n",
    "for s in STATE[\"skills\"]:\n",
    "    nm = s.get(\"name\")\n",
    "    if nm:\n",
    "        resume_skill_names.append(nm)\n",
    "    for a in s.get(\"aliases\") or []:\n",
    "        resume_skill_names.append(a)\n",
    "resume_skill_names = _unique([_clean_token(x) for x in resume_skill_names if x])\n",
    "\n",
    "strong_hits = set()\n",
    "partial_hits = set()\n",
    "skill_map = {}\n",
    "\n",
    "opts = STATE[\"options\"]\n",
    "for r in resume_skill_names:\n",
    "    if not canon_pool:\n",
    "        break\n",
    "    best, sc = _best_match(r, canon_pool)\n",
    "    if best is None: \n",
    "        continue\n",
    "    skill_map[r] = {\"canon\": best, \"score\": sc}\n",
    "    if sc >= opts[\"match_strong\"]:\n",
    "        strong_hits.add(best)\n",
    "    elif sc >= opts[\"match_partial\"]:\n",
    "        partial_hits.add(best)\n",
    "\n",
    "STATE[\"canon\"][\"skill_map\"] = skill_map\n",
    "STATE[\"canon\"][\"normalized_skills\"] = resume_skill_names\n",
    "STATE[\"canon\"][\"normalized_required\"] = canon_pool[:len(jd_req)] if canon_pool else []\n",
    "STATE[\"canon\"][\"normalized_preferred\"] = canon_pool[len(jd_req):] if canon_pool else []\n",
    "\n",
    "# ---------- 3) JD alignment (required/preferred present/partial/missing) ----------\n",
    "def _align_list(targets: list[str]) -> list[dict]:\n",
    "    out = []\n",
    "    for t in targets:\n",
    "        t_c = _clean_token(t)\n",
    "        status = \"missing\"\n",
    "        if t_c in strong_hits:\n",
    "            status = \"present_strong\"\n",
    "        elif t_c in partial_hits:\n",
    "            status = \"present_partial\"\n",
    "        # find any one evidence phrase from STATE[\"skills\"]\n",
    "        ev, cid = None, None\n",
    "        if status != \"missing\":\n",
    "            for s in STATE[\"skills\"]:\n",
    "                names = [s.get(\"name\",\"\")] + (s.get(\"aliases\") or [])\n",
    "                for nm in names:\n",
    "                    if _best_match(nm, [t_c])[1] >= opts[\"match_partial\"]:\n",
    "                        if s.get(\"evidence\"):\n",
    "                            ev = s[\"evidence\"][0]\n",
    "                        if s.get(\"chunk_ids\"):\n",
    "                            cid = s[\"chunk_ids\"][0]\n",
    "                        break\n",
    "                if ev or cid:\n",
    "                    break\n",
    "        out.append({\"name\": t_c, \"status\": status, \"evidence\": ev, \"chunk_id\": cid})\n",
    "    return out\n",
    "\n",
    "STATE[\"jd_alignment\"][\"required\"]  = _align_list(jd_req)\n",
    "STATE[\"jd_alignment\"][\"preferred\"] = _align_list(jd_pref)\n",
    "\n",
    "missing_required = [x[\"name\"] for x in STATE[\"jd_alignment\"][\"required\"] if x[\"status\"] == \"missing\"]\n",
    "\n",
    "# ---------- 4) Responsibilities overlap ----------\n",
    "jd_resps = STATE[\"jd_snapshot\"].get(\"responsibilities\", []) or []\n",
    "all_highlights = []\n",
    "for r in STATE[\"timeline\"]:\n",
    "    for h in r.get(\"highlights\") or []:\n",
    "        all_highlights.append(h)\n",
    "for p in STATE[\"projects\"]:\n",
    "    if p.get(\"impact\"):\n",
    "        all_highlights.append(str(p[\"impact\"]))\n",
    "\n",
    "resp_hits = 0\n",
    "for r in jd_resps:\n",
    "    r_clean = _clean_token(r)\n",
    "    found = any(r_clean in _clean_token(h) for h in all_highlights)\n",
    "    resp_hits += 1 if found else 0\n",
    "resp_cover = (resp_hits / max(1, len(jd_resps))) if jd_resps else 0.0\n",
    "\n",
    "# ---------- 5) Evidence depth ----------\n",
    "evidence_count = _count_numbers(all_highlights)\n",
    "# normalize: 0 numbers -> 0, 10+ numbers -> ~1.0\n",
    "evidence_depth = min(1.0, evidence_count / 10.0)\n",
    "\n",
    "# ---------- 6) Seniority (rough) ----------\n",
    "# Try to infer required years from JD title or text: e.g., \"Senior\", \"5+ years\", etc.\n",
    "jd_text_all = (STATE[\"raw\"].get(\"jd_text\") or \"\") + \" \" + (STATE[\"jd_snapshot\"].get(\"title\") or \"\")\n",
    "m_years = re.search(r\"(\\d+)\\s*\\+?\\s*(?:years|yrs)\", jd_text_all.lower())\n",
    "req_years = int(m_years.group(1)) if m_years else None\n",
    "\n",
    "seniority_fit = 1.0\n",
    "if years_experience is not None and req_years is not None:\n",
    "    # simple ramp: fully fit if >= req_years, else linearly scale\n",
    "    seniority_fit = max(0.0, min(1.0, years_experience / max(1, req_years)))\n",
    "\n",
    "# ---------- 7) Education/cert presence ----------\n",
    "has_degree = any(e.get(\"degree\") for e in STATE[\"education\"])\n",
    "has_cert   = bool(STATE.get(\"certs\"))\n",
    "edu_cert_score = 0.0\n",
    "if has_degree:\n",
    "    edu_cert_score += 0.5\n",
    "if has_cert:\n",
    "    edu_cert_score += 0.5\n",
    "\n",
    "# ---------- 8) Compute coverage primitives ----------\n",
    "req_total = len(jd_req)\n",
    "req_strong = sum(1 for x in STATE[\"jd_alignment\"][\"required\"] if x[\"status\"] == \"present_strong\")\n",
    "req_partial = sum(1 for x in STATE[\"jd_alignment\"][\"required\"] if x[\"status\"] == \"present_partial\")\n",
    "\n",
    "pref_total = len(jd_pref)\n",
    "pref_strong = sum(1 for x in STATE[\"jd_alignment\"][\"preferred\"] if x[\"status\"] == \"present_strong\")\n",
    "pref_partial = sum(1 for x in STATE[\"jd_alignment\"][\"preferred\"] if x[\"status\"] == \"present_partial\")\n",
    "\n",
    "STATE[\"coverage\"] = {\n",
    "    \"required\": {\"total\": req_total, \"strong\": req_strong, \"partial\": req_partial, \"missing\": len(missing_required)},\n",
    "    \"preferred\": {\"total\": pref_total, \"strong\": pref_strong, \"partial\": pref_partial},\n",
    "    \"responsibility_overlap\": resp_cover,     # 0..1\n",
    "    \"seniority_fit\": seniority_fit,           # 0..1\n",
    "    \"evidence_depth\": evidence_depth,         # 0..1\n",
    "    \"edu_cert\": edu_cert_score,               # 0..1 (in 0.5 steps)\n",
    "}\n",
    "\n",
    "# ---------- 9) Score (0â€“100) with caps/penalties ----------\n",
    "rubric = {\n",
    "    \"must_have_coverage\": 35,       # strong matches weighted higher than partial\n",
    "    \"preferred_alignment\": 15,\n",
    "    \"experience_seniority_fit\": 20,\n",
    "    \"responsibility_overlap\": 10,\n",
    "    \"evidence_depth\": 10,\n",
    "    \"education_cert_match\": 10,\n",
    "}\n",
    "# must-have coverage: strong=1.0, partial=0.5\n",
    "req_cov = 0.0\n",
    "if req_total > 0:\n",
    "    req_cov = (req_strong + 0.5 * req_partial) / req_total\n",
    "\n",
    "pref_cov = 0.0\n",
    "if pref_total > 0:\n",
    "    pref_cov = (pref_strong + 0.5 * pref_partial) / pref_total\n",
    "\n",
    "score = (\n",
    "    rubric[\"must_have_coverage\"] * req_cov +\n",
    "    rubric[\"preferred_alignment\"] * pref_cov +\n",
    "    rubric[\"experience_seniority_fit\"] * seniority_fit +\n",
    "    rubric[\"responsibility_overlap\"] * resp_cover +\n",
    "    rubric[\"evidence_depth\"] * evidence_depth +\n",
    "    rubric[\"education_cert_match\"] * edu_cert_score\n",
    ")\n",
    "\n",
    "# penalties / caps\n",
    "opts = STATE[\"options\"]\n",
    "if missing_required:\n",
    "    score = min(score, float(opts[\"cap_if_must_have_missing\"]))\n",
    "# (Optional) degree penalty if JD explicitly asks for a degree and none found:\n",
    "if (\"bachelor\" in jd_text_all.lower() or \"degree\" in jd_text_all.lower()) and not has_degree:\n",
    "    score = max(0.0, score - float(opts[\"degree_penalty\"]))\n",
    "\n",
    "score_100 = int(round(max(0.0, min(100.0, score))))\n",
    "\n",
    "# ---------- 10) Reasons & gaps (deterministic text) ----------\n",
    "strong_list = [x[\"name\"] for x in STATE[\"jd_alignment\"][\"required\"] if x[\"status\"] == \"present_strong\"]\n",
    "partial_list = [x[\"name\"] for x in STATE[\"jd_alignment\"][\"required\"] if x[\"status\"] == \"present_partial\"]\n",
    "pref_strong_list = [x[\"name\"] for x in STATE[\"jd_alignment\"][\"preferred\"] if x[\"status\"] == \"present_strong\"]\n",
    "\n",
    "reasons = []\n",
    "if strong_list:\n",
    "    reasons.append(f\"Strong matches on required: {', '.join(strong_list[:6])}\" + (\"â€¦\" if len(strong_list) > 6 else \"\"))\n",
    "if partial_list:\n",
    "    reasons.append(f\"Partial matches on required: {', '.join(partial_list[:6])}\" + (\"â€¦\" if len(partial_list) > 6 else \"\"))\n",
    "if pref_strong_list:\n",
    "    reasons.append(f\"Preferred skills present: {', '.join(pref_strong_list[:6])}\" + (\"â€¦\" if len(pref_strong_list) > 6 else \"\"))\n",
    "if years_experience is not None:\n",
    "    if req_years:\n",
    "        reasons.append(f\"Experience: {years_experience} yrs vs JD ~{req_years} yrs (fit={round(100*seniority_fit)}%).\")\n",
    "    else:\n",
    "        reasons.append(f\"Experience: {years_experience} yrs (JD years not specified).\")\n",
    "reasons.append(f\"Responsibilities overlap: {int(round(100*resp_cover))}% ; Evidence signals: {evidence_count} metrics found.\")\n",
    "\n",
    "risk_flags = []\n",
    "if missing_required:\n",
    "    risk_flags.append(f\"Missing required: {', '.join(missing_required[:6])}\" + (\"â€¦\" if len(missing_required) > 6 else \"\"))\n",
    "if not STATE[\"contacts\"].get(\"email\"):\n",
    "    risk_flags.append(\"No email detected.\")\n",
    "if years_experience is None:\n",
    "    risk_flags.append(\"Could not infer experience years.\")\n",
    "if evidence_count == 0:\n",
    "    risk_flags.append(\"No quantifiable impact statements detected.\")\n",
    "\n",
    "# ---------- 11) Persist final ----------\n",
    "STATE[\"final\"] = {\n",
    "    \"score_100\": score_100,\n",
    "    \"breakdown\": {\n",
    "        \"must_have_coverage\": round(rubric[\"must_have_coverage\"] * req_cov, 2),\n",
    "        \"preferred_alignment\": round(rubric[\"preferred_alignment\"] * pref_cov, 2),\n",
    "        \"experience_seniority_fit\": round(rubric[\"experience_seniority_fit\"] * seniority_fit, 2),\n",
    "        \"responsibility_overlap\": round(rubric[\"responsibility_overlap\"] * resp_cover, 2),\n",
    "        \"evidence_depth\": round(rubric[\"evidence_depth\"] * evidence_depth, 2),\n",
    "        \"education_cert_match\": round(rubric[\"education_cert_match\"] * edu_cert_score, 2),\n",
    "    },\n",
    "    \"reasons\": reasons,\n",
    "    \"strong_matches\": strong_list,\n",
    "    \"skill_gaps\": missing_required,\n",
    "    \"risk_flags\": risk_flags,\n",
    "}\n",
    "\n",
    "Path(STATE[\"artifacts\"][\"base_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "with open(STATE[\"artifacts\"][\"paths\"][\"final_json\"], \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(STATE[\"final\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[ok] Final score: {STATE['final']['score_100']}/100\")\n",
    "print(\"[ok] Breakdown:\", STATE[\"final\"][\"breakdown\"])\n",
    "print(\"[ok] Reasons:\", *STATE[\"final\"][\"reasons\"], sep=\"\\n  - \")\n",
    "print(\"[ok] Risk flags:\", *STATE[\"final\"][\"risk_flags\"], sep=\"\\n  - \")\n",
    "print(f\"[ok] Saved â†’ {STATE['artifacts']['paths']['final_json']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d81121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
